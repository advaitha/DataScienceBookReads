[
  {
    "objectID": "datamesh_in_action/02_chapter.html",
    "href": "datamesh_in_action/02_chapter.html",
    "title": "DataScienceBookReads",
    "section": "",
    "text": "key Decision Drivers are:-\n\nBusiness need\nsocio-technically complex organization\nData Maturity\nEngineering Maturity\nA data mesh is for companies with complex domains\nA data mesh is for companies with diverse data sources\nData Governance maturity - Easier to implement in companies with higher data governance maturity\nData mesh is easier to implement in companies in which software is developed in close cooperation with data experts\nData mesh is easier to implement in companies that are following a domain-driven design approach\n\n\n\n\nAre we planning to be data driven?\nA data mesh requires a business case and is useful when there are related complex needs\n\n\n\n\n\nA data mesh is for socio-technically complex organization with complex data needs\n\n\n\n\nFour stages of Data Maturity\n\nDescriptive - what happened?\nDiagnostic - why it happened?\nPredictive - what will happen?\nPrescriptive - How to take advantage of what will happen?\n\nA data mesh is for companies with a certain level of data maturity.\n\n\n\n\n\nLevel of test automation\nDevOps and CI/CD culture\nLevel of embedding security into the development process\nA data mesh is for companies with a high level of software engineering maturity\n\n\n\n\n\n\nA data mesh is for companies with complex domains\nA data mesh is for companies with diverse data sources\n\n\n\n\n\n\n\n\nIt divides governance maturity into six tiers:\n\nUnaware\nAware\nReactive\nProactive\nManaged\nEfficient\n\n\n\n\n\n\n\n\nSuitable in the following cases:- * Most of the data sources are structured * Well known use cases and not likely to expand * You extract value by using BI or reporting * Small number of unchanging source systems * Central team has expertise in data warehousing\n\n\n\n\nSuitable in the following cases:- * Deal with big data * Structured, semi-structured, and unstructured data sources * Hard to foresee all the data use cases * Data sources owned by small number of teams * consumption patterns are not yet established \n\n\n\nFeatures of a lakehouse:- * ACID support * schema enforcement and governance * BI support on top of the same data source\n\nSuitable in the following cases:- * Deal with big data * Structured, semi-structured, and unstructured data sources * Hard to foresee all the data use cases * Data sources owned by small number of teams * consumption patterns are not yet established\nThe elements of a data mesh may be constructed with existing infrastructure\n\n\n\nThis is a technology centric solution to data management.It provides a low-code or no-code platform to connect data sources with consumers alongside governance applied on top of it.\n\nSuitable in the following cases:- * You have structured, semi-structured, and unstructured data sources. * You are dealing with big data. * There are many data sources, and they are owned by many teams. * You are dealing with many different ways to consume the data.\n\n\n\n\n\n\nData Mesh Implementation Process\n\n\n\n\n\nData Mesh enabling and platform team\n\n\n\n\n\nData Mesh and Data Governance\n\n\n\n\n\n\nChoosing a business goal and enable with data\nDefine the data products needed\nDevelop data product\nCollect feedback about the platform and analyze shared needs\nEstablish common policies and improve governance\nDevelop the platform\n\nIt should connect data products and data consumers\nAutomating the implementation of governance policies"
  },
  {
    "objectID": "writing_for_ds.qmd/summary.html",
    "href": "writing_for_ds.qmd/summary.html",
    "title": "DataScienceBookReads",
    "section": "",
    "text": "The key takeaway is that you should publish a few quality blogs before trying to monetize or get writing jobs.\n\n\n\nAn article should offer a solution to the problem. It should focus on the unique way to minimize the pain point to the user\nIn case you are unsure, discuss the article with the editor\nWrite article in the active voice - Subject should be performing an action\nShort paragraphs with 5-6 lines\nHighlight important keywords\nInclude table of contents\nUse heading and sub-headings\nSecond person vs. first person (not clear)\nUse minimum number of words\nUse meaningful description for cross-references\nAvoid overused datasets\nBeginners always look for articles that show full data science life cycle.\nIf possible document how the dataset was collected\nPublish a post after proofreading it\nTechnical terms should be spelled correctly - Python and not python\nstick to one English version throughout the article\nAsk for feedback before publishing\nDo fact-checking\nAim for depth when explaining technical concepts\nProvide link to the tools\nAlways use images with creative common license and always mention the source\nUse visualization\nStart with an outline\nResearch first\n\n\n\n\n\nCreate awareness for the company\nGrow a community\nLead generation\nEducation Align your goals with the company goals when you are creating content for them.\n\n\n\n\n\nEvaluate why the rejection happened\nAsk for feedback in case of a rejection\nDon’t take feedback personally\nGet advice and feedback from other Datascience writers\nTry self-publish\nBe consistent and create writing samples\n\n\n\n\n\nKeep a list of ideas to write\nLook at other blogs for inspiration\nWrite on something which was not clear or missing in a blog post\nDocument a project you are working on\nUse google trends\nUltimate guides for new technology\nShare learnings from a data science event you attended\nSummarize a data science book you have read\nDevelopers love “from scratch” implementations\n\n\n\n\n\nAvoid self-plagiarism (signing a contract and using the content elsewhere)\nProper referencing\nMake most of your content original\nuse open-source images\nFree image sources - Unsplash, Pixabay. Reference the image source\nCite datasets\nReference code\n\n\n\n\n\nLink to authentic websites\nLink to other related blog posts\nUse canonical URLs if reposting a blog on other sites. This is notifying search engines of the original posts.\nUpdate articles to the latest version\n\n\n\n\n\nNewsletters\nScientific papers\nFollowing experts on social media\nConferences and webinars\nPodcasts\nTalk to experts\n\n\n\n\n\nCompanies which pay for writing\n\nHeartbeat\nneptune.ai\nweights and biases\nreal python\npaperspace\nDigital ocean\nAuthO\nNimbleBox\n\nGoogle - “data science publications that pay writers”\n\n\n\n\n\nHacker News - Guidelines on how to do this\nReddit groups\n\ndatascience\nlanguageTechnology\nmachinelearning\ncomputervision\nlearnmachinelearning\n\nsyndicate the content with canonical urls on other platforms\nlinkedin groups\nvideo tutorial for the same article and publish on youtube\nNewsletters - Email data science newsletter curators and ask them to feature your post\n\n\n\n\n\nGrammarly\nMedium\nGhost\nEzgif and Tinywow - Create GIFs from videos\nOcoya - Scheduling posts on social media\n\n\n\n\n* Machine Learning Developer Advocate\n* Machine Learning Educator\n* Technical Documentation Manager\n* Technical Writer\n* Machine Learning Community Builder\n* DevRel Manager\n* Developer Evangelist\n* Machine Learning Advocate Engineer\n* Developer Advocate Engineer    \n\nWhen applying for these roles - Send a list of articles along with their engagement numbers - Metrics on the posts\n\n\n\n\n\nWriting short ebooks and publishing them on sites like\n\nLeanpub\nSmashwords\nGumroad"
  },
  {
    "objectID": "datamesh_in_action/01_chapter.html",
    "href": "datamesh_in_action/01_chapter.html",
    "title": "DataScienceBookReads",
    "section": "",
    "text": "Data Mesh is a decentralized paradigm. It decentralizes the ownership of data, its transformation into information and its serving.\nData Mesh is guided by four principles\n\nDomain ownership\nDomain data as a product\nFederated computational governance\nSelf-serve data platform\n\n\n\n\n\nCentralized team creates an organizational bottleneck\nMonolithic data storage creates a technological bottleneck\nData quality and data ownership are implicitly assigned which causes confusion\n\n\n\n\n\nData and relevant components should be owned, maintained and developed by the people closest to it.\nData should have clear ownership and it should not be on the centralized level of an organization\nOrganizational structure resembles business domains\n\n\n\n\n\nData offered by a team should follow typical product features like:-\n\nViable quality\nAnticipation of user needs\nSecured availability\nFocus on user goals\nFindable\nInteroperable - Different data products should be combinable in a way that increases their value\n\nData product owner should deliver a data product addressing the business problem based on predefined success criteria\n\n### Federated computational governance * Federate and automate data governance across all participants of the data mesh\n\n\n\n\nEfforts that are duplicated throughout the company to a larger extent can be packaged into a platform\nData can be accessed through a self-serve data infrastructure\nData can be accessed through a self-serve data infrastructure\nFeatures that can be supported by the platform are:-\n\nGovernability\nSecurity\nFlexibility, adaptability and elasticity\nResilience\nProcess automation\n\n\n\n\n\n\nconway's law - Any organization that designs a system will produce a design whose structure is a copy of the organization’s communication structure.\nData Mesh is both technical and organisational solution\n\n\n\n\n\n\nAvailability of tooling for data domain teams and central platform data team\nImpact on cost\nRedundancy in the data\nCross-domain analytics is difficult\nCo-ordinated efforts from multiple engaged parties\nGoing from data as a by-product to data as a produt is a substantial cultural shift"
  },
  {
    "objectID": "ai_at_the_edge/chapter_2.html",
    "href": "ai_at_the_edge/chapter_2.html",
    "title": "DataScienceBookReads",
    "section": "",
    "text": "Greenfield project - Building the hardware and software from the ground up for a specific purpose.\nBrownfield project- Using the existing hardware to bring new capabilities.\nPreventing forest fires using power line fault detection - Izoelektro’s RAM-1 device uses edge AI to solve the problem.\nProtecting firefigthers with intelligent wearables. BioTrac Band is a wearable device designed for workers, like firefighters who are exposed to extreme conditions.\nElephantEdge is an open source project to create a tracking collar to help understand elephant behavior.\n\n\n\n\n\nKeep track of objects\nUnderstanding and controlling systems\nUnderstanding people and living things"
  },
  {
    "objectID": "ai_at_the_edge/chapter_1.html",
    "href": "ai_at_the_edge/chapter_1.html",
    "title": "DataScienceBookReads",
    "section": "",
    "text": "Embedded systems are meant to perform one specific, dedicated task.\nThe data obtained from sensors tends to have a very high volume but a relatively low informational content.\nSend large volumes to low-value information is very costly and energy intensive. Due to this a majority of data collected by IoT sensors are usually discarded.\nEdge AI - Instead of sending data off to some distant location for processing, decisions are taken locally. Only send important information to the servers.\n\n\n\n\nCloud ane Edge Continnum\n\n\n\nDigital Signal Processing (DSP) is the practice of using algorithms to manipulate the data captured by embedded systems. DSP is used to modify signals which are fed to ML systems.\n\n\n\n\nBLERP\n\nBandwidth\nLatency\nEconomics\nReliability\nPrivacy\n\n\n\n\n\nEdge AI is ususally used for inference. It is rare to train models on the edge devices. Edge devices will lack labeled data which will be required for training.\nFocus of edge AI is on sensor data. DSP is a critical part of most edge AI deployments.\nML models can get very small.\nLearning from feedback is limited.\nCompute is diverse and heterogeneous - Microcontrollers, System-on-chip (SoC), FPGAs etc\n“Good enough” is often the goal\nTools and best practices are still evolving"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A website for Data Science Book Summaries."
  },
  {
    "objectID": "network_science_with_python/01.html",
    "href": "network_science_with_python/01.html",
    "title": "DataScienceBookReads",
    "section": "",
    "text": "Language can be transformed into networks and analyzed\n\n\n\n\nText can be used to extract relationships. Networks can be used to study those relationships"
  },
  {
    "objectID": "data_lake.html",
    "href": "data_lake.html",
    "title": "DataScienceBookReads",
    "section": "",
    "text": "Data lakes are powerful tools for storing, managing and analyzing vast amounts of data from a variety of sources. They promise to provide businesses with invaluable insights that can help drive growth and innovation. However, like any powerful tool, data lakes can also pose significant risks if not managed properly. Without careful planning and management, a data lake can quickly become a “data swamp,” filled with irrelevant, low-quality, and outdated data that can harm business operations and hinder decision-making. In this blog post, we will explore the common causes of data swamps in data lakes and provide practical tips and best practices to help you avoid the risks and ensure your data lake remains a valuable asset for your organization.\nBefore discussing data swamps, let’s take a look at what are the benefits of a data lake.\n\n\nThere are several benefits to using a data lake, including:\nCentralized data storage: Data lakes provide a centralized location for storing all data, making it easier to access and analyze.\nScalability: Data lakes can store and process petabytes of data, making them ideal for businesses that generate large amounts of data.\nFlexibility: Data lakes can store structured, semi-structured, and unstructured data, allowing businesses to store and analyze data from a variety of sources.\nCost-effectiveness: Data lakes are typically more cost-effective, as they typically store raw data in object stores like s3 or blob storage.\nImproved data analysis: Data lakes enable businesses to apply a wide range of analytics techniques to their data, including machine learning, artificial intelligence, and natural language processing. This enables businesses to gain insights and make more informed decisions.\nOverall, data lakes provide businesses with a powerful tool for storing, processing, and analyzing large amounts of data, enabling better decision-making and business outcomes.\n\n\n\nData lakes are like Gardens. What makes a Garden either attractive or unattractive to us?\nA garden is a thing of beauty, a place to escape the hustle and bustle of everyday life and connect with nature. However, just like any other beautiful thing, a garden requires constant maintenance to keep it looking its best. Without regular care, a garden can quickly become overgrown, unkempt, and unsightly. Weeds will sprout up where they’re not wanted, flowers will wither and die, and before long, the once-beautiful garden will be unrecognizable. But with a little bit of attention and care, a garden can thrive, with vibrant flowers, lush greenery, and a peaceful ambience that can soothe the soul. Data lakes need care and maintenance just like a beautiful garden.\nHere are some reasons why a data lake becomes a data swamp:-\n\nLack of data governance: Without proper data governance policies and procedures, a data lake can become cluttered with irrelevant or inaccurate data, making it difficult to find and use the data that is needed.\nPoor data quality: If data is ingested into the data lake without being properly validated or cleaned, it can lead to poor data quality.\nLack of metadata management: Metadata is data that describes the data, and it is essential for understanding and using the data in the data lake. Without proper metadata management, it can be difficult to understand what data is stored in the data lake and how it should be used.\nLack of data access controls: If the data lake is not properly secured, unauthorized users may be able to access and modify data, which can lead to inaccurate or irrelevant data being stored in the data lake.\nLack of privacy controls: If Personally Identifiable Information (PII) data is not handled properly, it can lead to data leakage and loss of user privacy. This can severely damage the reputation of the organization.\n\n\n\n\nSimply put, taking regular care and maintenance can be a great way to prevent a data lake from becoming a data swamp.\nHere are some ways to maintain a data lake:-\n\nEstablishing proper data governance policies to ensure that data is properly managed throughout its lifecycle.\nEnsure data quality during the processing of the raw data. Take necessary action to ensure raw data is clean.\nImplement Metadata management and data discovery, your users should be able to find the required data in the data lake and also know how it was generated and what various attributes mean.\nEnsure data access controls, such that only authorized users can access and modify the data.\nEnsure user privacy is protected, never store PII data in the data lake. Ensure PII data is either masked or any appropriate privacy controls are in place. If there is no business case for using PII data then better not to store it in the first place.\n\nThese are easier said than done. It needs a lot of time, effort and coordination with different teams to ensure data lakes are crystal clear and beneficial to the organisation.\nData lakes are not the only solution to handle large amounts of an organisation’s data. There are other alternatives to data lakes like Enterprise Data Warehouse, Data Mart, Data Virtualization, Data Fabric, Data Hub and Data Mesh. Every organization should evaluate the requirements, strengths and weaknesses of each framework and choose the best solution. Let us discuss these frameworks in some of our future blog posts.\nTo Summarize, data lakes are capable to handle a variety of data sources in huge volumes. They are flexible and scalable. When they are planned, built and maintained properly, they can be very beneficial for any organization. If not they will become data swamps. A few ways to prevent a data lake from becoming a data swamp are ensuring data quality, governance, metadata management, data discovery, data access control, and protecting user privacy.\nHope you learned something about data lakes today and let us meet in our next blog post."
  },
  {
    "objectID": "datamesh_in_action/03_chapter.html",
    "href": "datamesh_in_action/03_chapter.html",
    "title": "DataScienceBookReads",
    "section": "",
    "text": "A mesh is a graph consisting of nodes and edges\n\n\n\n\nPrepare a System landscape diagram\nStake holder mapping - Attitude, Power, Interest\nChoosing development teams\nChoosing a data governance team\nSetting up MVP Governance\nDeveloping minimal data products\n\nIdentifying domain-oriented datasets\nChoose data product owners\nDecide on the minimum viable data product description\nDeveloping the simplest tools to expose your data\nSetting up the minimal platform\nEnsuring platform security\n\n\n\n\n\nstakeholder mapping and management"
  }
]